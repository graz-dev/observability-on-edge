# Maritime Vessel Observability - Edge Computing Demo

A production-realistic observability demo for **edge computing in maritime environments**, showcasing intelligent trace sampling, persistent queuing for network resilience during satellite connectivity loss, and unified visualization of correlated vessel telemetry data.

**Perfect for demonstrating**: IoT monitoring, remote site observability, maritime operations, or any edge deployment with intermittent connectivity.

## ğŸ¯ What This Demo Shows

- **Intelligent Sampling**: Tail-based sampling reduces trace data by 80-90% while keeping all errors and slow requests
- **Network Resilience**: Persistent file queues prevent data loss during network outages
- **Full Correlation**: Seamlessly navigate between metrics, logs, and traces using trace IDs and exemplars
- **Edge-Optimized**: Lightweight collectors designed for resource-constrained environments
- **Production Patterns**: Real-world configuration of OpenTelemetry, Prometheus, Loki, and Jaeger

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VESSEL (EDGE) â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€ SHORE HUB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                  â”‚         â”‚                            â”‚
â”‚  Vessel Monitor (Go + OTel)      â”‚         â”‚  Grafana                   â”‚
â”‚  - Engine sensors                â”‚         â”‚     â†‘                      â”‚
â”‚  - Navigation data               â”‚         â”‚  Prometheus, Loki, Jaeger  â”‚
â”‚  - Diagnostics                   â”‚         â”‚                            â”‚
â”‚         â†“                        â”‚         â”‚                            â”‚
â”‚  OTel Collector                  â”‚  â•â•â•â–¶   â”‚  [Unified Dashboard]       â”‚
â”‚  â€¢ Tail-based sampling           â”‚  sat    â”‚                            â”‚
â”‚  â€¢ Persistent queues             â”‚  link   â”‚  View all vessel data:     â”‚
â”‚  â€¢ Batching                      â”‚         â”‚  - Engine health           â”‚
â”‚         â†‘                        â”‚         â”‚  - GPS tracks              â”‚
â”‚  Fluent Bit (logs)               â”‚         â”‚  - Diagnostic reports      â”‚
â”‚                                  â”‚         â”‚  - System alerts           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

```bash
# macOS
brew install docker k3d kubectl k6

# Linux
# Install Docker, k3d, kubectl, k6 (see DEMO.md for details)

# Windows
choco install docker-desktop k3d kubernetes-cli k6
```

### Setup (5 minutes)

```bash
# 1. Clone the repository
git clone <repo-url>
cd observability-on-edge

# 2. Run the setup script
./scripts/setup.sh

# 3. Access the dashboards
open http://localhost:30300  # Grafana (admin/admin)
open http://localhost:30686  # Jaeger
```

### Run Demo Scenarios

#### Scenario 1: Normal Operation
```bash
# Generate realistic traffic
./scripts/load-generator.sh

# Open Grafana dashboards:
# - Application Observability
# - Monitoring System Health
```

#### Scenario 2: Satellite Link Loss & Recovery
```bash
# Start vessel monitoring (simulates continuous sensor data)
./scripts/load-generator.sh

# In another terminal, simulate satellite connectivity loss
./scripts/simulate-network-failure.sh

# Watch persistent queue grow in Grafana dashboard
# Vessel continues collecting data locally

# After 30-60 seconds, restore satellite connection
./scripts/restore-network.sh

# Observe: Queue drains, all vessel data syncs to shore hub!
# No sensor data lost during the outage
```

### Cleanup

```bash
./scripts/cleanup.sh
```

## ğŸ“Š Components

| Component | Purpose | Image |
|-----------|---------|-------|
| **Vessel Monitor** | Maritime telemetry system | Custom Go app |
| **OTel Collector** | Sampling & collection | `otel/opentelemetry-collector-contrib:0.95.0` |
| **Fluent Bit** | Log collection | `fluent/fluent-bit:2.2` |
| **Jaeger** | Trace storage & UI | `jaegertracing/all-in-one:1.54` |
| **Prometheus** | Metrics storage | `prom/prometheus:v2.49.1` |
| **Loki** | Log aggregation | `grafana/loki:2.9.4` |
| **Grafana** | Unified visualization | `grafana/grafana:10.3.3` |

## ğŸ“ Key Features

### Tail-Based Sampling

Smart sampling that waits for complete traces before deciding:
- âœ… Keep **all** error traces
- âœ… Keep traces slower than **200ms**
- âœ… Sample **10%** of normal traces
- ğŸ“‰ Result: **80-90% data reduction**

### Network Resilience

Persistent file queues in OTel Collector:
- Queue data during network outages
- Automatic retry with exponential backoff
- Drain queued data when connectivity returns
- **Zero data loss** during intermittent connectivity

### Full Observability Correlation

- **Logs â†’ Traces**: Click trace_id in logs to view trace in Jaeger
- **Metrics â†’ Traces**: Exemplars link metric data points to traces
- **Traces â†’ Logs**: View related logs from trace timeline
- **Unified View**: Single Grafana interface for all signals

## ğŸ“š Documentation

- **[DEMO.md](DEMO.md)** - Comprehensive guide with:
  - Detailed architecture explanation
  - Component configuration details
  - Data flow diagrams
  - Step-by-step demo scenarios
  - Dashboard usage guide
  - Troubleshooting tips

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ app/                          # Go application with OTel
â”‚   â”œâ”€â”€ main.go                   # Application entry point
â”‚   â”œâ”€â”€ handlers.go               # HTTP handlers
â”‚   â”œâ”€â”€ telemetry.go              # OTel instrumentation
â”‚   â”œâ”€â”€ go.mod                    # Go dependencies
â”‚   â””â”€â”€ Dockerfile                # Container image
â”‚
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ otel-collector-config.yaml
â”‚   â”œâ”€â”€ fluentbit-config.yaml
â”‚   â”œâ”€â”€ prometheus-config.yaml
â”‚   â”œâ”€â”€ loki-config.yaml
â”‚   â””â”€â”€ grafana-*.yaml
â”‚
â”œâ”€â”€ k8s/                          # Kubernetes manifests
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ edge-node/                # Edge workloads
â”‚   â”‚   â”œâ”€â”€ app-deployment.yaml
â”‚   â”‚   â”œâ”€â”€ otel-collector-*.yaml
â”‚   â”‚   â””â”€â”€ fluentbit-*.yaml
â”‚   â”œâ”€â”€ hub-node/                 # Hub workloads
â”‚   â”‚   â”œâ”€â”€ jaeger-*.yaml
â”‚   â”‚   â”œâ”€â”€ prometheus-*.yaml
â”‚   â”‚   â”œâ”€â”€ loki-*.yaml
â”‚   â”‚   â””â”€â”€ grafana-*.yaml
â”‚   â”œâ”€â”€ grafana-dashboards/       # Pre-built dashboards
â”‚   â”‚   â”œâ”€â”€ app-observability.json
â”‚   â”‚   â””â”€â”€ monitoring-health.json
â”‚   â””â”€â”€ network-policy-deny.yaml  # For network failure simulation
â”‚
â”œâ”€â”€ load-tests/                   # Load testing
â”‚   â””â”€â”€ k6-script.js              # k6 load test scenarios
â”‚
â”œâ”€â”€ scripts/                      # Automation scripts
â”‚   â”œâ”€â”€ setup.sh                  # Full setup
â”‚   â”œâ”€â”€ simulate-network-failure.sh
â”‚   â”œâ”€â”€ restore-network.sh
â”‚   â”œâ”€â”€ load-generator.sh
â”‚   â””â”€â”€ cleanup.sh
â”‚
â”œâ”€â”€ DEMO.md                       # Comprehensive demo guide
â””â”€â”€ README.md                     # This file
```

## ğŸ” Access Services

### Always Available (NodePort)

**Grafana**:
- **URL**: http://localhost:30300
- **Username**: `admin`
- **Password**: `admin`
- **Dashboards**:
  - Application Observability
  - Monitoring System Health

**Jaeger**:
- **URL**: http://localhost:30686
- **Features**:
  - Trace search and visualization
  - Service dependency graph
  - Trace comparison

### Other Services (Port-forward Required)

**Prometheus**:
```bash
kubectl port-forward -n observability svc/prometheus 9090:9090
# http://localhost:9090
```

**Loki** (via Grafana):
```bash
# Already accessible through Grafana datasource
```

## ğŸ¯ Use Cases

### Primary Scenario: Maritime Vessel Monitoring
This demo simulates a **vessel monitoring system** running on a boat with satellite connectivity:
- âš“ **Engine sensors**: RPM, temperature, oil pressure (fast, continuous polling)
- ğŸ§­ **Navigation data**: GPS, speed, heading, depth (fast, high-frequency)
- ğŸ”§ **Diagnostics**: Complex engine analysis (slow, resource-intensive)
- ğŸš¨ **System alerts**: Sensor failures and warnings (error-prone)

**The Challenge**: When the vessel loses satellite connection (common at sea), the monitoring system must:
- âœ… Continue collecting all sensor data locally
- âœ… Queue data in persistent storage
- âœ… Sync everything when connectivity returns
- âœ… Reduce bandwidth using intelligent sampling (keep errors + slow diagnostics, drop fast normal reads)

### Other Edge Computing Scenarios
The same patterns apply to:
- ğŸ­ **Industrial IoT**: Factory equipment monitoring with unreliable network
- ğŸª **Retail PoS**: Point-of-sale systems in remote stores
- ğŸ›¢ï¸ **Oil & Gas**: Remote drilling site monitoring
- ğŸšœ **Agriculture**: Smart farming equipment telemetry
- ğŸ“¡ **Any edge deployment** with intermittent connectivity and bandwidth constraints

## ğŸ› ï¸ Customization

### Adjust Sampling Threshold

Edit `configs/otel-collector-config.yaml`:
```yaml
processors:
  tail_sampling:
    policies:
      - name: latency-policy
        type: latency
        latency:
          threshold_ms: 200  # â† Change this
```

### Add Custom Endpoints

Edit `app/handlers.go` and add new handlers:
```go
func (s *Server) myNewHandler(w http.ResponseWriter, r *http.Request) {
    // Your code here
}
```

Register in `app/main.go`:
```go
http.HandleFunc("/api/new", server.tracingMiddleware(server.myNewHandler))
```

### Modify Load Test

Edit `load-tests/k6-script.js` to change:
- Virtual user count
- Test duration
- Request mix
- Custom scenarios

## ğŸ› Troubleshooting

### Pods Not Starting
```bash
kubectl get pods -n observability
kubectl describe pod <POD_NAME> -n observability
```

### No Data in Grafana
```bash
# Check datasources
kubectl get pods -n observability | grep -E "jaeger|prometheus|loki"

# Check OTel Collector logs
kubectl logs -n observability deployment/otel-collector
```

### Network Policy Not Working
```bash
# Verify policy exists
kubectl get networkpolicy -n observability

# Test connectivity
kubectl exec -n observability deployment/otel-collector -- wget -O- http://jaeger:16686 --timeout=5
```

See [DEMO.md](DEMO.md) for detailed troubleshooting.

## ğŸ“– Learn More

- **OpenTelemetry**: https://opentelemetry.io/docs/
- **Tail-Based Sampling**: https://opentelemetry.io/docs/collector/configuration/#tailsamplingprocessor
- **Grafana Correlations**: https://grafana.com/docs/grafana/latest/fundamentals/correlations/
- **k3s/k3d**: https://k3d.io/

## ğŸ¤ Contributing

Contributions welcome! Areas for enhancement:
- Additional application endpoints
- More sampling policies
- Advanced correlation examples
- Multi-cluster scenarios
- Production hardening guides

## ğŸ“ License

This project is provided as-is for educational and demonstration purposes.

## ğŸ™ Acknowledgments

Built with:
- OpenTelemetry Community
- Grafana Labs (Grafana, Loki)
- Prometheus Community
- Jaeger Project
- Fluent Bit Community
- k3s/k3d Projects

---

**Ready to explore edge observability?**

Start with `./scripts/setup.sh` and read [DEMO.md](DEMO.md) for the full experience! ğŸš€
